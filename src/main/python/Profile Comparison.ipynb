{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "#path to net analyis of each dataset\n",
    "net_birth = \"../../../eval-results/_server-results/net-BP-birth/net.eval\" \n",
    "net_uni = \"../../../eval-results/_server-results/net-BP-uni/net.eval\" \n",
    "net_sap = \"../../../eval-results/_server-results/net-BP-sap/net.eval\" \n",
    "\n",
    "#path to bp results\n",
    "bp_birth = \"../../../eval-results/_server-results/test13_2/\"\n",
    "bp_uni = \"../../../eval-results/_server-results/test17_2/\"\n",
    "bp_sap = \"../../../eval-results/_server-results/test18_2/\"\n",
    "\n",
    "#path to alpha results\n",
    "alpha_birth = \"../../../eval-results/_server-results/test22_2/\"\n",
    "alpha_uni = \"../../../eval-results/_server-results/test21_2/\"\n",
    "alpha_sap = \"../../../eval-results/_server-results/test23_2/\"\n",
    "\n",
    "#path to bpp results\n",
    "bpp_birth = \"../../../eval-results/_server-results/test26_2/\"\n",
    "bpp_uni = \"../../../eval-results/_server-results/test24_2/\"\n",
    "bpp_sap = \"../../../eval-results/_server-results/test25_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net1adder(row):\n",
    "    #get net names\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['Name'].split('-')[0].replace(\".pnml\",\"\")\n",
    "    else:\n",
    "        return 0\n",
    "def net2adder(row):        \n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['Name'].split('-')[1].split('.')[0].replace(\".pnml\",\"\")\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def pnml_remover(row):\n",
    "    #get net names\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['Name'].replace(\".pnml\",\"\").replace(\".rdf\",\"\")\n",
    "    else:\n",
    "        return row['Name']\n",
    "\n",
    "def net1NonTaus(row):\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return df_nets.at[row['net1'],'nNonSilentTransitions'] \n",
    "    else:\n",
    "        return 0\n",
    "def net2NonTaus(row):\n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return df_nets.at[row['net2'],'nNonSilentTransitions'] \n",
    "    else:\n",
    "        return 0\n",
    "def sumNonTaus(row):    \n",
    "    if row['Name'] != \"Aggregated (MICRO)\" and row['Name'] != \"Aggregated (MACRO)\":\n",
    "        return row['net1NonTaus'] + row['net2NonTaus']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def fetch_data(folder, net_analysis, dataset, profile):\n",
    "    dir_list = next(os.walk(folder))[1]\n",
    "    dfs = []\n",
    "    #merge all \n",
    "    for subfolder in dir_list:\n",
    "        #print(subfolder)\n",
    "        evalFile = folder +\"/\" + subfolder +\"/aggRetrospectiveResults.eval\"\n",
    "        confFile = folder +\"/\" + subfolder +\"/config.log\"\n",
    "        if os.path.exists(evalFile):\n",
    "            df = pd.read_csv(evalFile ,encoding=\"ISO-8859-1\", skipinitialspace=True)\n",
    "            with open(confFile) as json_file:\n",
    "                conf = json.load(json_file)\n",
    "                #add config information to dataframe \n",
    "                df['matcher'] = conf['matcher']['ilp'] + \" - \" + conf['matcher']['profile'] +\" - \" + str(conf['matcher']['word-sim']) + \" - sim-weight=\" + str(conf['matcher']['sim-weight']) + \" - match-postprocessing=\" + str(conf['matcher']['postprocessing-thresh'] ) + \" - Node Limit: \" +str(conf['matcher']['ilp-node-limit']) + \"- Time Limit : \" +str(conf['matcher']['ilp-time-limit'])\n",
    "                df['matcher_wo_weight'] = conf['matcher']['ilp'] + \" - \" + conf['matcher']['profile'] +\" - \" + str(conf['matcher']['word-sim']) + \" - match-postprocessing=\" + str(conf['matcher']['postprocessing-thresh']) + \" - Node Limit: \" +str(conf['matcher']['ilp-node-limit']) + \"- Time Limit : \" +str(conf['matcher']['ilp-time-limit'])\n",
    "                df['matcher_wo_limit'] = conf['matcher']['ilp'] + \" - \" + conf['matcher']['profile'] +\" - \" + str(conf['matcher']['word-sim']) + \" - sim-weight=\" + str(conf['matcher']['sim-weight']) + \" - match-postprocessing=\" + str(conf['matcher']['postprocessing-thresh'] )\n",
    "                df['complex-matches'] = conf['matcher']['complex matches']\n",
    "                df['profile'] = conf['matcher']['profile']\n",
    "                df['ilp'] =  conf['matcher']['ilp']\n",
    "                df['word-sim'] =  conf['matcher']['word-sim']\n",
    "                df['sim-weight'] = conf['matcher']['sim-weight']\n",
    "                df['matcher-postprocessing-threshold'] = conf['matcher']['postprocessing-thresh']\n",
    "                df['eval-postprocessing-threshold'] = conf['evaluation']['postprocessing-thresh']\n",
    "                df['ILP-time-limit'] = conf['matcher']['ilp-time-limit']\n",
    "                df['ILP-node-limit'] = conf['matcher']['ilp-node-limit']\n",
    "            \n",
    "            #df.set_index(['Name','matcher'])\n",
    "            dfs.append(df)\n",
    "\n",
    "    df_combined = pd.concat(dfs)\n",
    "\n",
    "    #convert time\n",
    "    df_combined['OVERALL TIME'] = df_combined['OVERALL TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['BP TIME'] = df_combined['BP TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['LABEL-SIM TIME'] = df_combined['LABEL-SIM TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['LP TIME'] = df_combined['LP TIME'].map(lambda x: x / 1000000000.)\n",
    "    df_combined['Dataset'] = dataset\n",
    "    df_combined['Profile'] = profile\n",
    "\n",
    "    #extend with net information stored in net_analysis\n",
    "    df_nets = pd.read_csv(net_analysis ,encoding=\"ISO-8859-1\", skipinitialspace=True)\n",
    "    #df_nets.set_index('Name')\n",
    "\n",
    "\n",
    "    df_combined['net1'] = df_combined.apply(lambda row: net1adder(row), axis=1)\n",
    "    df_combined['net2'] = df_combined.apply(lambda row: net2adder(row), axis=1)\n",
    "    #df_combined['net1NonTaus'] = df_combined.apply(lambda row: net1NonTaus(row), axis=1)\n",
    "    #df_combined['net2NonTaus'] = df_combined.apply(lambda row: net2NonTaus(row), axis=1)\n",
    "    #df_combined['sumNonTaus'] = df_combined.apply(lambda row: sumNonTaus(row), axis=1)\n",
    "    df_combined['Name'] = df_combined.apply(lambda row: pnml_remover(row), axis=1)\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "bp = fetch_data(bp_birth, net_birth ,\"Birth\", \"BP\")\n",
    "bp = bp.append(fetch_data(bp_uni, net_uni, \"Uni\", \"BP\"))\n",
    "bp = bp.append(fetch_data(bp_sap, net_sap, \"Sap\", \"BP\"))\n",
    "\n",
    "alpha = fetch_data(alpha_birth, net_birth ,\"Birth\", \"Alpha\")\n",
    "alpha = alpha.append(fetch_data(alpha_uni, net_uni, \"Uni\", \"Alpha\"))\n",
    "alpha = alpha.append(fetch_data(alpha_sap, net_sap, \"Sap\", \"Alpha\"))\n",
    "\n",
    "bpp = fetch_data(bpp_birth, net_birth ,\"Birth\", \"BP+\")\n",
    "bpp = bpp.append(fetch_data(bpp_uni, net_uni, \"Uni\", \"BP+\"))\n",
    "bpp = bpp.append(fetch_data(bpp_sap, net_sap, \"Sap\", \"BP+\"))\n",
    "\n",
    "#Fetch label sim\n",
    "lo = fetch_data(bp_birth, net_birth ,\"Birth\", \"Label only\")\n",
    "lo = lo.append(fetch_data(bp_uni, net_uni, \"Uni\", \"Label only\"))\n",
    "lo = lo.append(fetch_data(bp_sap, net_sap, \"Sap\", \"Label only\"))\n",
    "lo = lo[lo[\"sim-weight\"] == 0]\n",
    "\n",
    "# concat all three datasets\n",
    "df_all = pd.concat([bp,alpha,bpp])\n",
    "\n",
    "# filter label only out (w=0)\n",
    "df_all = df_all[df_all[\"sim-weight\"] > 0]\n",
    "\n",
    "#add lo\n",
    "df_all = df_all.append(lo)\n",
    "\n",
    "df_macro_avg = df_all[df_all.Name == \"Aggregated (MACRO)\"]\n",
    "df_micro_avg = df_all[df_all.Name == \"Aggregated (MICRO)\"]\n",
    "\n",
    "#export dataframe to excel\n",
    " with pd.ExcelWriter('profile_comparison.xlsx') as writer:  \n",
    "    df_macro_avg.to_excel(writer, sheet_name='MACRO')  \n",
    "    df_micro_avg.to_excel(writer, sheet_name='MICRO')  \n",
    "\n",
    "#select max fscore for each profile on each dataset\n",
    "df_macro_max_fscore = df_macro_avg.groupby(['Dataset','Profile']).max()[\"FSCORE\"]\n",
    "df_macro_max_fscore = df_macro_max_fscore.reset_index()\n",
    "\n",
    "df_micro_max_fscore = df_micro_avg.groupby(['Dataset','Profile']).max()[\"FSCORE\"]\n",
    "df_micro_max_fscore = df_micro_max_fscore.reset_index()\n",
    "\n",
    "#print(df_macro_max_fscore)\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(18.5, 5.25)\n",
    "sns.catplot(x=\"Dataset\", y=\"FSCORE\", hue=\"Profile\", data=df_macro_max_fscore, kind=\"bar\", height=4, aspect=2, ax = ax1, palette=sns.color_palette(\"Reds_d\", 4),order=[\"Birth\",\"Uni\",\"Sap\"], hue_order =[\"Label only\",\"Alpha\",\"BP\",\"BP+\"])\n",
    "ax1.set_ylabel(\"Maximal Fscore\")\n",
    "ax1.set_title(\"Macro\")\n",
    "ax1.set_ylim([0.0,0.75])\n",
    "\n",
    "sns.catplot(x=\"Dataset\", y=\"FSCORE\", hue=\"Profile\", data=df_micro_max_fscore, kind=\"bar\", height=4, aspect=2, ax = ax2, palette=sns.color_palette(\"Reds_d\", 4),order=[\"Birth\",\"Uni\",\"Sap\"], hue_order =[\"Label only\",\"Alpha\",\"BP\",\"BP+\"])\n",
    "ax2.set_ylabel(\"Maximal Fscore\")\n",
    "ax2.set_title(\"Micro\")\n",
    "ax2.set_ylim([0.0,0.75])\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(18.5, 5.25)\n",
    "sns.catplot(x=\"Dataset\", y=\"FSCORE\", hue=\"Profile\", data=df_macro_avg, kind=\"bar\", height=4, aspect=2, ax = ax1, palette=sns.color_palette(\"Reds_d\", 4), order=[\"Birth\",\"Uni\",\"Sap\"], hue_order =[\"Label only\",\"Alpha\",\"BP\",\"BP+\"])\n",
    "ax1.set_ylabel(\"Average Fscore\")\n",
    "ax1.set_title(\"Macro\")\n",
    "ax1.set_ylim([0.0,0.75])\n",
    "\n",
    "sns.catplot(x=\"Dataset\", y=\"FSCORE\", hue=\"Profile\", data=df_micro_avg, kind=\"bar\", height=4, aspect=2, ax = ax2, palette=sns.color_palette(\"Reds_d\", 4),order=[\"Birth\",\"Uni\",\"Sap\"], hue_order =[\"Label only\",\"Alpha\",\"BP\",\"BP+\"])\n",
    "ax2.set_ylabel(\"Average Fscore\")\n",
    "ax2.set_title(\"Micro\")\n",
    "ax2.set_ylim([0.0,0.75])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "bp = fetch_data(bp_birth, net_birth ,\"Birth\", \"BP\")\n",
    "bp = bp.append(fetch_data(bp_uni, net_uni, \"Uni\", \"BP\"))\n",
    "bp = bp.append(fetch_data(bp_sap, net_sap, \"Sap\", \"BP\"))\n",
    "\n",
    "alpha = fetch_data(alpha_birth, net_birth ,\"Birth\", \"Alpha\")\n",
    "alpha = alpha.append(fetch_data(alpha_uni, net_uni, \"Uni\", \"Alpha\"))\n",
    "alpha = alpha.append(fetch_data(alpha_sap, net_sap, \"Sap\", \"Alpha\"))\n",
    "\n",
    "bpp = fetch_data(bpp_birth, net_birth ,\"Birth\", \"BP+\")\n",
    "bpp = bpp.append(fetch_data(bpp_uni, net_uni, \"Uni\", \"BP+\"))\n",
    "bpp = bpp.append(fetch_data(bpp_sap, net_sap, \"Sap\", \"BP+\"))\n",
    "\n",
    "#Fetch label sim\n",
    "lo = fetch_data(bp_birth, net_birth ,\"Birth\", \"Label only\")\n",
    "lo = lo.append(fetch_data(bp_uni, net_uni, \"Uni\", \"Label only\"))\n",
    "lo = lo.append(fetch_data(bp_sap, net_sap, \"Sap\", \"Label only\"))\n",
    "lo = lo[lo[\"sim-weight\"] == 0]\n",
    "\n",
    "# concat all three datasets\n",
    "df_all = pd.concat([bp,alpha,bpp])\n",
    "\n",
    "# filter label only out (w=0)\n",
    "df_all = df_all[df_all[\"sim-weight\"] > 0]\n",
    "df_all = df_all[df_all[\"eval-postprocessing-threshold\"] <0.9]\n",
    "\n",
    "#add lo\n",
    "df_all = df_all.append(lo)\n",
    "\n",
    "#filter out aggregations\n",
    "df_all = df_all[(df_all[\"Name\"] != \"Aggregated (MACRO)\") & (df_all[\"Name\"] != \"Aggregated (MICRO)\")]\n",
    "\n",
    "#filter on pp threshold\n",
    "df_all = df_all[df_all[\"eval-postprocessing-threshold\"] == 0.5]\n",
    "\n",
    "#filter for each dataset\n",
    "df_all = df_all[df_all[\"Dataset\"] == \"Uni\"]\n",
    "\n",
    "#sorting according to log\n",
    "ordering = df_all[df_all[\"Profile\"] == \"BP+\"].sort_values(\"FSCORE\")[\"Name\"]\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1)\n",
    "fig.set_size_inches(9.25, 5.25)\n",
    "sns.pointplot(x=\"Name\", y=\"FSCORE\", hue=\"Profile\", data=df_all, ax = ax1, palette= sns.color_palette(\"Reds_d\", 3), sort=False, order = ordering, markers=\"\");\n",
    "#ax1.set_xticks([])\n",
    "ax1.set_ylabel(\"Fscore\")\n",
    "ax1.set_xlabel(\"Pair of models\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
